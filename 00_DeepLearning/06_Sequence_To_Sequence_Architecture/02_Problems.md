# Encoder-Decoder (Seq2Seq) Architecture and Its Problems

## ğŸ”¹ Overview

- Discussing **encoder-decoder sequence-to-sequence architecture**
- Understanding its problems and how to fix them using **attention mechanism**

---

## ğŸ”¹ Encoder Structure

- Uses **LSTM (Long Short-Term Memory)** for sequence processing
- Each time step (t=1,2,3,4) takes an input and generates a hidden state
- Uses an **embedding layer** to convert words into vector representations

### ğŸ“Œ Key Components of the Encoder

1. **Embedding Layer** â†’ Converts words into dense vectors
2. **LSTM Layers** â†’ Process sequences and generate hidden states
3. **Context Vector (C)** â†’ Encapsulates sentence meaning into a fixed-size vector

---

## ğŸ”¹ Decoder Structure

- Takes the **context vector** from the encoder
- Generates output step-by-step using **LSTM layers**
- Uses **Softmax activation** to predict the next word
- The generated output is fed back into the decoder for the next prediction

---

## ğŸ”¹ Problem with Seq2Seq Architecture

### âŒ **Context Vector Bottleneck**

- The **entire sentence is represented by a single context vector (C)**
- Works well for **short sentences**, but fails for **longer sentences**
- The **BLEU score (performance metric for translation accuracy) drops** as sentence length increases

### ğŸ” Why Does This Happen?

- The last hidden state (context vector) **retains more information** about the **recent words**
- Words from earlier in the sentence **lose importance** in long sequences
- This leads to **poor performance** on long text sequences

---

## ğŸ”¹ Solution: **Attention Mechanism**

- Introduces **additional context** instead of relying only on the **final context vector**
- Improves the modelâ€™s ability to focus on **relevant words** in long sentences
- Uses a **weighted sum of hidden states** instead of a single context vector

---

## ğŸ”¹ Introduction to Attention

- Based on the research paper **"Neural Machine Translation by Jointly Learning to Align and Translate"**
- Uses **Bidirectional LSTM** instead of a single-direction LSTM
- Allows decoder to focus on **important words dynamically** at each timestep

### ğŸ¯ **Key Changes with Attention**

1. Instead of a **single context vector**, multiple context vectors are used
2. The decoder gets **more information from different parts of the sentence**
3. Improves accuracy for **longer sentences**

---

## ğŸ”¹ Next Steps

- In the next session, we will **deep dive into the Attention Mechanism**
- Understand **how it modifies the Encoder-Decoder architecture**
- Explore the **architecture diagram and implementation details**

---

## ğŸ¯ Summary

âœ… **Problem:** Traditional Seq2Seq models struggle with **long sentences**  
âœ… **Reason:** The context vector loses **early sentence information**  
âœ… **Solution:** **Attention Mechanism** dynamically assigns weights to different words  
âœ… **Next Topic:** Understanding **how Attention improves Seq2Seq models**

---

ğŸ“Œ **Stay tuned for the next session on Attention Mechanism!** ğŸ¥
