# Transformer Architecture: Layer Normalization & Batch Normalization

## ğŸ“ Introduction to the Series

Nitesh welcomes viewers to the deep learning playlist focused on the **Transformer architecture**. He shares that the series has covered embeddings, attention mechanisms, and positional encoding, and the current focus is on the **final piece of the architecture: normalization**, specifically **Layer Normalization**.

---

## ğŸ”‘ Key Components of Transformers

- **Embeddings**
- **Self-Attention & Multi-Head Attention**
- **Positional Encoding**
- **Layer Normalization** â† **Current Focus**

---

## ğŸ“š What is Normalization?

Normalization is the process of transforming data to have specific statistical properties. In deep learning, it helps:

- Stabilize training
- Speed up convergence
- Prevent issues like vanishing/exploding gradients

### ğŸ§® Types of Normalization

- **Standardization**: Subtract mean, divide by std deviation
- **Min-Max Scaling**: Scale between fixed min & max range

### ğŸ“Œ Normalization Targets:

- **Inputs** (features like f1, f2)
- **Activations** of **hidden layers**

---

## âœ… Benefits of Normalization

1. Training becomes stable
2. Faster convergence
3. Mitigates **Internal Covariate Shift**
4. Acts as a form of **Regularization**

### ğŸ” Internal Covariate Shift:

- During training, weight updates cause changes in layer activations' distributions
- Causes unstable training and poor generalization

---

## ğŸ§ª Batch Normalization (BN)

### ğŸ’¡ Concept:

- BN normalizes activations across **batch dimension**
- Mean and variance are computed **per feature** over the batch

### âš™ï¸ Steps:

1. Compute weighted sums \( z_1, z_2, z_3, \ldots \)

2. Normalize using mean \( \mu \) and standard deviation \( \sigma \):

   $$
   z_{\text{norm}} = \frac{z - \mu}{\sigma}
   $$

3. Scale and shift using learnable parameters \( \gamma \), \( \beta \):
   $$
   z_{\text{scaled}} = \gamma \cdot z_{\text{norm}} + \beta
   $$

### âŒ Why BN Fails in Transformers:

- Transformers process **sequential data** (varying length)
- Padding causes skewed stats (means, std dev)
- BN fails when multiple sequences have different lengths and padding

---

## âœ… Layer Normalization (LN)

### ğŸ’¡ Concept:

- LN normalizes **per data point**, **across features**
- Better suited for sequences and attention mechanisms

### âš™ï¸ Steps:

1. For each vector:

   $$
   \mu = \frac{1}{d} \sum_{i=1}^{d} x_i,\quad
   \sigma = \sqrt{\frac{1}{d} \sum_{i=1}^{d}(x_i - \mu)^2}
   $$

2. Normalize:

   $$
   x' = \frac{x - \mu}{\sigma}
   $$

3. Apply learnable scale and shift:
   $$
   \text{LN}(x) = \gamma \cdot x' + \beta
   $$

### âœ… Why LN is Better for Transformers:

- No dependence on batch size
- Handles padding effectively
- Doesnâ€™t get affected by sentence length

### ğŸ“Œ Visualization:

Given input embedding of shape `(seq_len Ã— d_model)`:

- **BatchNorm** computes stats across batch dimension (bad for padded sequences)
- **LayerNorm** computes stats across feature dimension (independent of padding)

---

## ğŸ” Batch vs Layer Normalization

| Feature                   | Batch Norm      | Layer Norm        |
| ------------------------- | --------------- | ----------------- |
| Normalizes over           | Batch dimension | Feature dimension |
| Sensitive to batch size?  | âœ… Yes          | âŒ No             |
| Handles padding well?     | âŒ No           | âœ… Yes            |
| Suitable for Transformers | âŒ No           | âœ… Yes            |

---

## ğŸ”š Conclusion

- **Layer Normalization** is preferred in Transformer architectures
- It ensures **stable training**, handles **padded sequences**, and works **independently of batch size**
- The series now transitions into applying these components to understand the **complete Transformer architecture**.

ğŸ‘‰ If you're interested in the decoder or further architecture walkthrough, stay tuned for the next video.
