# ðŸ§  Transformer Encoder Architecture â€” Masterclass Notes

---

## ðŸ“Œ Overview

- **Instructor**: Nitesh (YouTube Channel)
- **Focus**: **Encoder** part of the Transformer.
- **Approach**: Focuses on **core components first** before explaining the full model.
- **Source**: Inspired by the seminal paper â€” _â€œAttention is All You Needâ€ (Vaswani et al., 2017)_.

---

## ðŸ§­ Why Transformers?

Transformers revolutionized deep learning for sequential data by:

- Removing recurrence (used in RNNs/LSTMs)
- Using **attention mechanisms** to model **global dependencies**
- Enabling parallelization for faster training

---

## ðŸ“¦ High-Level Transformer Architecture

       Input Sentence
            â†“
        Encoder Stack
            â†“
         Decoder Stack
            â†“
      Final Predictions

- ðŸ”¹ **Encoder**: Reads and encodes input sequence
- ðŸ”¸ **Decoder**: Generates output (e.g., translation, summary)
- Transformer = Stack of N encoder + N decoder blocks (typically N = 6)

---

## ðŸ§± Components of the Encoder Block

Each encoder block contains:

1. **Multi-Head Self-Attention**
2. **Add & Layer Normalization**
3. **Feed-Forward Neural Network (FFN)**
4. **Add & Layer Normalization**

Each of the 6 encoder blocks has:

- **Same architecture**
- **Unique weights and parameters**

---

## âœï¸ Step-by-Step: Encoding the Sentence `"How are you"`

---

### 1. ðŸ”¤ Input Preprocessing

#### a. **Tokenization**

- Sentence â†’ Tokens: `"How"`, `"are"`, `"you"`

#### b. **Embedding Layer**

- Converts tokens into **dense vectors** (e.g., 512-dimensional)
- These embeddings are **learned** during training

#### c. **Positional Encoding**

- Injects **order information** into the model (since attention is order-agnostic)
- Formula involves **sinusoidal functions**:
- Final input = Word Embedding + Positional Encoding

---

### 2. ðŸ§  Inside One Encoder Block

#### ðŸ”¹ A. Multi-Head Self-Attention

- Allows the model to **focus on different positions** (context)
- Each attention head performs:
  $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
- **Queries (Q)**, **Keys (K)**, and **Values (V)** are computed via linear projections of input
- Outputs from all heads are concatenated and projected:
  $\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_8) \cdot W^O$

#### ðŸ”¹ D. Add & Layer Norm (Post-FFN)

- Adds input of FFN to its output
- Normalized to ensure stability
  $y_1 = \text{LayerNorm}(z_1 + \text{FFN}(z_1))$

---

## ðŸ” Repeating Encoder Blocks

- Input vector goes through **6 such blocks** (can vary)
- Output vector shape is preserved: **[sequence_length, d_model]** = [3, 512]
- Final encoder output â†’ passed to the decoder

---

## ðŸ” Why Residual Connections?

| Benefit                        | Explanation                                                 |
| ------------------------------ | ----------------------------------------------------------- |
| ðŸ”„ Prevent vanishing gradients | Helps gradients flow backward across deep networks          |
| ðŸ§© Retain original features    | If transformation is poor, identity path helps              |
| ðŸš€ Faster convergence          | Empirically shown to improve training speed and performance |

Origin: Introduced in **ResNet** to tackle deep network degradation.

---

## ðŸ§  Why Feed-Forward Neural Networks?

- Attention is a **linear operator** âž lacks expressive power
- FFNs add **non-linearity** to model complex features
- Research (2021): FFNs act like **key-value memory stores**:
  - Each FFN encodes distributional semantics
  - Stores pattern-matching structures seen during training

> **Fact**: FFNs contribute to **~2/3 of all transformer parameters**

---

## ðŸ“Š Final Encoder Output

| Token | Vector (512-d) | Interpretation                   |
| ----- | -------------- | -------------------------------- |
| How   | `yâ‚`           | Context-aware, normalized vector |
| are   | `yâ‚‚`           | Encodes position + semantics     |
| you   | `yâ‚ƒ`           | Ready for decoder processing     |

Shape: **[3 x 512]**

This matrix captures:

- **Word meaning**
- **Word position**
- **Contextual dependencies**

---

## ðŸ§© Why Stack Multiple Encoder Blocks?

| Reason                      | Description                                    |
| --------------------------- | ---------------------------------------------- |
| ðŸ§  Language is complex      | One layer is insufficient to model nuances     |
| ðŸ” Depth increases capacity | More layers capture abstract features          |
| ðŸ§ª Empirically optimal      | 6 layers found optimal in original Transformer |

> Deep stacking is a core principle in deep learning to **extract hierarchical features**.

---

## âœ… Summary

| Component            | Purpose                                       |
| -------------------- | --------------------------------------------- |
| Tokenization         | Breaks sentence into tokens                   |
| Embedding            | Converts tokens to 512-d vectors              |
| Positional Encoding  | Adds position info                            |
| Multi-Head Attention | Learns context-sensitive embeddings           |
| FFN                  | Introduces non-linearity                      |
| Residual + LayerNorm | Stabilizes training, preserves identity paths |
| Stacking 6 Layers    | Increases model depth & understanding         |

---

## ðŸš§ Coming Up Next

ðŸ”œ **Decoder Architecture**

- More complex due to:
  - Encoder-Decoder Attention
  - Masked Attention (for autoregressive decoding)
- Used for generation tasks (translation, summarization)

---

## ðŸ§  Advanced Thought Prompts

- What happens if we remove positional encoding?
- Can attention fully replace convolutions?
- How do different attention heads specialize?

---
