## Improving Neural Network Performance

1. _Vanishing Gradients_

   - Activation Functions
   - Weight Initialization

2. _Overfitting_

   - Reduce Complexity/Increase Data
   - Dropout Layers
   - Regularization (L1 & L2)
   - Early Stopping

3. _Normalization_

   - Normalizing inputs
   - Batch Normalization
   - Normalizing Activations

4. _Gradient Checking and Clipping_

5. _Optimizers_

   - Momentum
   - Adagrad
   - RMSprop
   - Adam

6. _Learning Rate Scheduling_

7. _Hyperparameter Tuning_

   - No. of Hidden Layers
   - Nodes/Layer
     - Batch Size
